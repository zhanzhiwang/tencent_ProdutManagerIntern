{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b56848",
   "metadata": {},
   "source": [
    "## 关联分析的定义\n",
    "\n",
    "关联是反映某个事物与其他事物之间的相互依存关系，关联分析是指在交易数据中，找出存在于项目集合之间的关联模式，即两个或者多个事物之间存在一定的关联性，则可以通过一个事物预测其他事物。换句话说，两项或者多项属性之间存在关联，就可以用其中一项的属性值预测另一项或另几项。例如描述牛奶和面包的关联规则（买面包的人也会买牛奶）：买面包-->买牛奶。关联关系的数学表达：\n",
    "$$X \\rightarrow Y, X \\cap Y = \\emptyset$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b8301",
   "metadata": {},
   "source": [
    "## 概念\n",
    "### 基本概念\n",
    "|订单编号 | 购买商品|\n",
    "| --- | --- |\n",
    "| 1 | 牛奶、面包、尿布 |\n",
    "| 2 | 可乐、面包、尿布、啤酒 |\n",
    "| 3 | 牛奶、尿布、啤酒、鸡蛋 |\n",
    "| 4 | 面包、牛奶、尿布、啤酒 |\n",
    "| 5 | 面包、牛奶、尿布、可乐 |\n",
    "\n",
    "- 事务：每条交易称为一个事务，上表就包含5个事务。\n",
    "- 项：交易的每个物品称为一个项，如面包、牛奶等。\n",
    "- 项集：包含零个或者多个项的集合叫做项集，例如{牛奶、面包}。\n",
    "- 规则：从众多项集中找出的各项之间的关系，如关联规则${牛奶} \\rightarrow {面包}$\n",
    "\n",
    "### 关联关系有效性指标\n",
    "- 支持度（support）\n",
    "- \n",
    "指某数据集中包含某几个特定项的事务出现的概率。例如，上面的表格中，$support(牛奶)=4/5$，$support({牛奶、面包})=3/5$。支持度反映了项集或规则的普遍程度。\n",
    "$$s_{X-Y} = \\frac{N(XY)}{N}$$\n",
    "其中$N(XY)$为同事购买X和Y的事务的数量，$N$为数据集总事务量。\n",
    "\n",
    "- 置信度（confidence）\n",
    "\n",
    "在给定前项X的前提下，后项Y发生的概率，是一个条件概率。例如，$confidence(买牛奶 \\rightarrow 买面包)= 3/4$.反映了规则的可靠度。\n",
    "$$c_{X \\rightarrow Y} = \\frac{N(XY)}{N(X)}$$\n",
    "\n",
    "- 提升度（lift）\n",
    "\n",
    "先购买某一商品对购买另一商品的提升作用。衡量了购买某一商品X时购买另一商品Y比单独购买商品Y的概率的提升程度。例如，$lift(买牛奶 \\rightarrow 买面包) = c(买牛奶 \\rightarrow 买面包)/s(买面包)$。用于判断商品组合方式是否具有实际价值。若$lift > 1$则表明有提升，组合有价值；若$lift = 1$则表明没有提升也没有下降；若$lift < 1$则表明有下降，组合无价值。\n",
    "$$l_{X \\rightarrow Y} = \\frac{c_{X \\rightarrow Y}}{s_{Y}}$$\n",
    "\n",
    "- 频繁项集\n",
    "\n",
    "若某一个项集的支持度大于预设的最小支持度（minsupport），则称这个项集为频繁项集。频繁项集也就是关联分析中所要找到的目标。\n",
    "\n",
    "<font color=\"red\">那么如何寻找频繁项集呢？当然我们可以根据上述概念描述的朴素方法寻找，但是数据量过大显然这个样子是不合适的。下面就介绍两个算法来实现寻找频繁项集。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598e9ce",
   "metadata": {},
   "source": [
    "## 算法实现\n",
    "### apriori算法\n",
    "核心思想：如果某个项集是频繁项集，那么它的子集也是频繁项集。逆否命题为：如果某个项集的子集不是频繁项集，那么这个项集也不是频繁项集。\n",
    "\n",
    "证明：\n",
    "$$s({A、B}) = \\frac{N(AB)}{N} \\leq \\frac{N(ABC)}{N} = s({A、B、C}) \\leq minsupport$$\n",
    "同理可得{A、B、C}的其他子集也满足上式。\n",
    "\n",
    "\n",
    "主要步骤：\n",
    "\n",
    "1. 根据数据集生成候选项，首先生成单项集。\n",
    "\n",
    "2. 设定最小支持度和最小置信度。\n",
    "\n",
    "3. 过滤掉数据项集中占比低于最小支持度的单项集中的项，只留下为频繁项集的单项集的项。\n",
    "\n",
    "4. 根据步骤3形成的新的数据集，进行项集之间的组合形成新的项集集合，再寻找频繁二项集。\n",
    "\n",
    "5. 重复步骤3、4，逐级寻找频繁三项集、四项集等，直到没有新的项集满足最小支持度。\n",
    "\n",
    "6. 根据步骤5形成的最终频繁项集，生成关联规则。\n",
    "\n",
    "7. 根据关联规则计算置信度，过滤掉小于最小置信度的关联规则。\n",
    "\n",
    "### apriori算法示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb28d1",
   "metadata": {},
   "source": [
    "#### 表格例子\n",
    "\n",
    "最小支持度设定为 0.6\n",
    "\n",
    "| TID | Items   |\n",
    "| --- | ------- |\n",
    "| T1  | A, B, C |\n",
    "| T2  | A, C    |\n",
    "| T3  | B, C    |\n",
    "| T4  | A, B    |\n",
    "| T5  | A, B, C |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "| Frequent 1-Itemset | Support |\n",
    "| ------------------ | ------- |\n",
    "| {A}                | **0.8** |\n",
    "| {B}                | **0.8** |\n",
    "| {C}                | **0.8** |\n",
    "\n",
    "频繁单项集：{A}{B}{C}\n",
    "\n",
    "---\n",
    "\n",
    "| Frequent 2-Itemset | Support |\n",
    "| ------------------ | ------- |\n",
    "| {A, B}             | **0.6** |\n",
    "| {A, C}             | **0.6** |\n",
    "| {B, C}             | **0.6** |\n",
    "\n",
    "频繁二项集：{A、B}{A、C}{B、C}\n",
    "\n",
    "---\n",
    "\n",
    "| Itemset   | Count | Support       |\n",
    "| --------- | ----- | ------------- |\n",
    "| {A, B, C} | 2     | 2/5 = **0.4** |\n",
    "\n",
    "无频繁三项集\n",
    "\n",
    "---\n",
    "频繁项集\n",
    "\n",
    "| Level | Frequent Itemsets   | Support（概率）   |\n",
    "| ----- | ------------------- | ------------- |\n",
    "| L1    | {A}, {B}, {C}       | 0.8, 0.8, 0.8 |\n",
    "| L2    | {A,B}, {A,C}, {B,C} | 0.6, 0.6, 0.6 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32fdcc",
   "metadata": {},
   "source": [
    "#### 不调用库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    # 定义一个示例数据集：每个子列表表示一笔事务（购买/出现的项集合）\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n",
    "    # 说明：这是用来测试算法的小数据集，实际使用时替换为真实事务列表\n",
    "\n",
    "\n",
    "def createC1(dataSet):\n",
    "    # 从数据集中生成 C1（所有单项的候选集合，形式为列表的列表）\n",
    "    C1 = []                       # 初始化空列表，用来存放单个项的列表形式（例如 [1], [2]）\n",
    "    for transaction in dataSet:   # 遍历每一笔事务\n",
    "        for item in transaction:  # 遍历事务中的每一个项\n",
    "            if not [item] in C1:  # 如果 [item]（单元素列表）还没有被加入 C1，则加入\n",
    "                C1.append([item]) # 避免重复加入同一单项\n",
    "    # 此时 C1 是一个类似 [[1], [3], [4], [2], [5]] 的列表（顺序可能不同）\n",
    "    C1.sort()                     # 对单项列表进行排序，便于生成候选集时保持稳定顺序\n",
    "    return list(map(frozenset, C1))\n",
    "    # 将每个单项列表转换为 frozenset（不可变集合），以便后续可以作为字典的键或集合操作\n",
    "    # map 返回迭代器，list() 转回列表。最终返回类似 [frozenset({1}), frozenset({2}), ...]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    \"\"\"\n",
    "    扫描数据库 D，统计候选集 Ck 的支持度，返回满足最小支持度的频繁项集列表（retList）和支持度字典（supportData）\n",
    "    参数：\n",
    "        D: 事务数据库，通常为 list of set 或 list of frozenset（这里假设已转换为集合形式）\n",
    "        Ck: 候选项集列表（元素为 frozenset），需要统计它们在 D 中的出现次数\n",
    "        minSupport: 最小支持度阈值（这里代码使用相对支持度，例如 0.5 表示 50%）\n",
    "    \"\"\"\n",
    "    ssCnt = {}            # 用来计数每个候选项集的出现次数（字典：项集 -> 次数）\n",
    "    for tid in D:         # 遍历数据库中的每一笔事务（每笔事务应为 set/frozenset）\n",
    "        for can in Ck:    # 遍历每个候选项集\n",
    "            if can.issubset(tid):  # 如果候选项集 can 是事务 tid 的子集（即该事务包含该候选项）\n",
    "                if not can in ssCnt:\n",
    "                    ssCnt[can] = 1  # 第一次计数则初始化为 1\n",
    "                else:\n",
    "                    ssCnt[can] += 1 # 否则在已有计数上加 1\n",
    "\n",
    "    numItems = float(len(D))    # 数据集中事务的总数（转换成 float，方便后面做除法得到相对支持度）\n",
    "    retList = []                # 用来保存满足最小支持度的频繁项集（以 frozenset 形式）\n",
    "    supportData = {}            # 保存所有候选项集的支持度值（项集 -> 支持度）\n",
    "\n",
    "    for key in ssCnt:           # 遍历所有被计数的候选项集\n",
    "        support = ssCnt[key] / numItems  # 计算相对支持度 = 出现次数 / 总事务数\n",
    "        if support >= minSupport:\n",
    "            retList.insert(0, key)       # 如果满足最小支持度，将其插入 retList（insert(0,...) 在列表头部插入）\n",
    "            # 注意：插入头部只是控制顺序，没有本质影响\n",
    "        supportData[key] = support      # 将该候选项集的支持度记录到 supportData\n",
    "\n",
    "    return retList, supportData     # 返回频繁项集列表和支持度字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1df0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprioriGen(Lk, k):  # creates Ck —— 用 L(k-1) 生成候选项集 Ck\n",
    "    retList = []        # 用来存储生成的候选项集 Ck\n",
    "    lenLk = len(Lk)     # 当前频繁项集 L(k-1) 的数量\n",
    "\n",
    "    # 两两组合 L(k-1) 中的项集，尝试生成 k 项集\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):  # 避免重复组合\n",
    "            L1 = list(Lk[i])[:k-2]     # 取第 i 个项集的前 k-2 个元素\n",
    "            L2 = list(Lk[j])[:k-2]     # 取第 j 个项集的前 k-2 个元素\n",
    "\n",
    "            L1.sort()                  # 排序以保证可比较\n",
    "            L2.sort()\n",
    "\n",
    "            # Apriori 剪枝：如果两个项集前 k-2 项相同，就可以合并生成 k 项集\n",
    "            if L1 == L2:               # 若前 k-2 个元素完全一样\n",
    "                retList.append(Lk[i] | Lk[j])  # 合并成 k 项集，使用集合的并操作 union\n",
    "\n",
    "    return retList                     # 返回生成的候选 k 项集 Ck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cfadb",
   "metadata": {},
   "source": [
    "#### 调用库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6dd8a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting efficient-apriori\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8a/f2/fe62e78214643e2b187fcbf7462dc17eb8071da2f04e1a3349a3297df799/efficient_apriori-2.0.6-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: efficient-apriori\n",
      "Successfully installed efficient-apriori-2.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install efficient-apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de8959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_apriori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1a150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    ['牛奶', '面包', '尿布'],\n",
    "    ['可乐', '面包', '尿布'],\n",
    "    ['牛奶', '尿布'],\n",
    "    ['牛奶', '面包'],\n",
    "    ['面包', '尿布'],\n",
    "    ['牛奶', '可乐', '面包', '尿布']\n",
    "]\n",
    "\n",
    "itemsets, rules = apriori(dataset, min_support=0.5, min_confidence=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8d77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {('牛奶',): 4, ('面包',): 5, ('尿布',): 5}, 2: {('尿布', '牛奶'): 3, ('尿布', '面包'): 4, ('牛奶', '面包'): 3}}\n",
      "[{牛奶} -> {尿布}, {面包} -> {尿布}, {尿布} -> {面包}, {牛奶} -> {面包}]\n"
     ]
    }
   ],
   "source": [
    "print(itemsets)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d4ab7",
   "metadata": {},
   "source": [
    "### apriori算法优缺点\n",
    "- 优点：容易上手\n",
    "- 缺点：面对大数据时运行速度太慢\n",
    "\n",
    "\n",
    "针对缺点再介绍一个新的算法 FP-growth算法（也叫FP树）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b05ac",
   "metadata": {},
   "source": [
    "### FP-growth算法\n",
    "\n",
    "核心思想：FP-growth主要采用一种分治的策略来解决该问题，我们可以用几个步骤来描述一下这种分治策略的大概步骤。\n",
    "\n",
    "主要步骤：\n",
    "\n",
    "1. 压缩数据集来表征每一个项，这个步骤一般是通过建立频繁模式树(frequent pattern tree，简称FP-tree)来实现的（其实就是字典树，很明显这是一种无损压缩方式）\n",
    "\n",
    "2. 统计每一个项在原数据集中出现的次数，并根据预先设定的support count去除低频项，然后根据出现次数对剩余项进行升序排序；从第一位项开始扫描频繁模式树的叶子节点，通过回溯得到每一个项对应的条件模式基(Condition Pattern Base，实则为叶子节点对应的路径集合)\n",
    "\n",
    "3. 根据条件模式基构建出条件频繁项集树(Conditional FP-tree，步骤和第一步完全一样，也就是根据条件模式基构建出一颗字典树)\n",
    "\n",
    "4. 根据条件FP-tree和support count得到最终的频繁项集\n",
    "\n",
    "<font color=\"red\">注意：对于FP-growth算法设定的是 support count是数量而不是概率 </font>\n",
    "\n",
    "### FP-growth算法实现\n",
    "比较复杂，有时间了再研究\n",
    "\n",
    "[点击参考知乎文章](https://zhuanlan.zhihu.com/p/411594391)\n",
    "\n",
    "[FP-growth树求解过程视频版](https://www.bilibili.com/video/BV1fX4y1A75f/?spm_id_from=333.337.search-card.all.click&vd_source=62a21f026fe60108faad8418b1f5996d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f40e1b",
   "metadata": {},
   "source": [
    "## 关联分析应用场景\n",
    "\n",
    "1. 电商推荐与购物篮分析。电商数据是大量的交易项集（订单多事一揽子商品），这样的数据就非常适合做关联分析。用找到的关联规则进行则和套餐推荐、关联商品促销、超市陈列优化等促进销售，从而提升经营利润。\n",
    "\n",
    "2. 内容推荐：看了某个或某类视频/文章的人还会看什么，进行关联推荐\n",
    "\n",
    "3. 金融风控：某些异常行为与诈骗之间的关联规则，有助于发现异常行为的组合模式，风险指标之间的关联。\n",
    "\n",
    "4. 用户反馈信息：在用户反馈信息中发现用户反映的问题之间的关联规则。\n",
    "\n",
    "5. 游戏运营分析：玩家行为分析，打副本的玩家也会强化武器和购买药品，有助于理解玩家行为模式，进行游戏商城推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69ec3f",
   "metadata": {},
   "source": [
    "## 关联分析优缺点\n",
    "\n",
    "\n",
    "### 一、关联分析的优点\n",
    "\n",
    "#### 1. 能挖掘潜在的关联关系，发现业务机会\n",
    "\n",
    "关联分析能自动发现用户行为或商品之间的共现关系，是寻找隐藏模式最有效的方法之一。\n",
    "\n",
    "例子：\n",
    "\n",
    "* 买「牛奶」的用户经常买「面包」\n",
    "* 跳到「支付失败」页面的用户更可能触发「联系客服」\n",
    "\n",
    "它适用于：推荐、商品组合、运营策略、功能共现分析等。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 可解释性强，业务易理解、可直接使用\n",
    "\n",
    "Apriori/FP-growth 的结果中有明确的：\n",
    "\n",
    "* 支持度（出现频率）\n",
    "* 置信度（在 A 的情况下出现 B 的概率）\n",
    "* 提升度（A→B 是否比随机更相关）\n",
    "\n",
    "业务方能直观看懂结果，因此更容易落地。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 可以基于简单事件日志快速构建\n",
    "\n",
    "不需要复杂建模，只要有行为序列或交易数据就能运行。\n",
    "适用于：\n",
    "\n",
    "* 行为事件流数据\n",
    "* 订单记录\n",
    "* 功能点击序列\n",
    "* 用户路径数据\n",
    "\n",
    "门槛低，速度快。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 支持多种业务应用场景\n",
    "\n",
    "例如：\n",
    "\n",
    "* 商品组合推荐（购物篮分析）\n",
    "* 功能共用关系挖掘\n",
    "* 用户群体行为模式识别\n",
    "* 运营活动组合优化\n",
    "* 流失路径中的高频共现事件识别\n",
    "\n",
    "业务应用面非常广。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 易于扩展与集成\n",
    "\n",
    "关联规则可直接用于：\n",
    "\n",
    "* 推荐系统候选集生成\n",
    "* 用户细分规则构造\n",
    "* 营销自动化策略（例如“做了 A 的用户推送 B”）\n",
    "\n",
    "实用性高。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、关联分析的缺点\n",
    "\n",
    "#### 1. 容易产生大量无意义规则，筛选成本高\n",
    "\n",
    "未加筛选时，可能会产生成百上千条规则，其中大部分：\n",
    "\n",
    "* 业务不相关\n",
    "* 符合直觉、没有价值\n",
    "* 是由共性行为引起的虚假关联\n",
    "\n",
    "需要人工筛选、设定阈值或使用提升度过滤。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 无法识别“因果关系”，只有“共现关系”\n",
    "\n",
    "关联分析只告诉你**A 和 B 经常一起发生**，但不会告诉你：\n",
    "\n",
    "* 是 A 导致 B\n",
    "* 是 B 导致 A\n",
    "* 还是 C 导致 A 和 B 同时发生\n",
    "\n",
    "例子：\n",
    "“买尿布的人也买啤酒”并不代表尿布导致啤酒销量上升。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 对稀疏数据不友好\n",
    "\n",
    "如果用户行为很分散，关联模式就会极其稀疏，可能出现：\n",
    "\n",
    "* 支持度很低\n",
    "* 规则稀少或不稳定\n",
    "* 即使有规则也不具备推广意义\n",
    "\n",
    "典型场景：大量功能、页面、商品品类。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 参数敏感：阈值（支持度/置信度）设定影响巨大\n",
    "\n",
    "* 支持度设高 → 规则少，但可能漏掉有价值的模式\n",
    "* 支持度设低 → 规则爆炸，噪音多\n",
    "* 置信度/提升度适配不同业务含义\n",
    "\n",
    "调参往往靠经验和业务理解，缺乏标准答案。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 难以处理复杂行为序列（顺序、时间依赖）\n",
    "\n",
    "标准关联分析只关注“是否一起出现”，忽略：\n",
    "\n",
    "* 行为顺序（A 后 B 是否更重要？）\n",
    "* 时间窗长短（5 分钟 vs 7 天）\n",
    "* 行为强度（频次高是否更重要？）\n",
    "\n",
    "对于时序关系，需要使用：\n",
    "\n",
    "* 序列模式挖掘（PrefixSpan）\n",
    "* Markov 链\n",
    "* 因果推断\n",
    "* 路径分析（Path Analysis）\n",
    "\n",
    "单纯关联分析的表达能力有限。\n",
    "\n",
    "---\n",
    "\n",
    "#### 总结：一句话理解关联分析\n",
    "\n",
    "> **关联分析擅长发现“是什么和什么一起发生”，但不擅长解释“为什么发生”。它的结果简单易懂，但需要结合业务筛选与解释，才能产生实际价值。**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784d5db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
